# ðŸš€ VoyageAI RAG Evaluation & Benchmarking Suite

A practical, opinionated framework for **benchmarking embeddings, vector search, and reranking strategies** on MongoDB Atlas using **VoyageAI** models.

This repo is designed for engineers who want **real answers** to questions like:

- *Do rerankers actually help my data?*
- *Is quantization â€œgood enoughâ€ â€” or am I losing recall?*
- *Where is my latency really going?*
- *What should I ship to production?*

If youâ€™re building a **RAG (Retrievalâ€‘Augmented Generation)** system and care about **quality Ã— latency Ã— cost**, this suite is for you.

---

## âœ¨ What This Repo Gives You

âœ” Repeatable, applesâ€‘toâ€‘apples experiments  
âœ” Sideâ€‘byâ€‘side comparisons of **Best vs Quantized (QAT)** embeddings  
âœ” Clear isolation of **retrieval vs reranking gains**  
âœ” Builtâ€‘in evaluation metrics (nDCG, Recall, MRR, MAP)  
âœ” Latency breakdowns (embed / search / rerank / total)  
âœ” Clean JSON outputs you can diff, graph, or feed into dashboards  

---

## ðŸ§ª Experiments Included

| # | Experiment | What It Tells You |
|--|--|--|
| **1** | Best Voyage (Direct) | Raw embedding quality & baseline latency |
| **2** | Best Voyage + Reranker | Maximum achievable quality |
| **3** | QAT Voyage + Reranker | Can quantization save money without hurting quality? |
| **4** | QAT Voyage (Direct) | Fastest & cheapest possible pipeline |

---

## ðŸ§  How to Think About the Results

- **Best â‰ˆ Best+Rerank** â†’ reranker may not be worth the cost  
- **QAT+Rerank â‰ˆ Best+Rerank** â†’ ðŸ”¥ *production winner*  
- **QAT Direct acceptable** â†’ ship the simplest thing  
- **Recall@50 drops hard** â†’ reranker canâ€™t recover missing docs  

---

## ðŸ“Š Metrics We Care About

**Retrieval Quality**
- nDCG@10 (ranking quality)
- Recall@50 (candidate coverage)
- MRR / MAP (optional)

**System Performance**
- Mean / Median / P95 latency
- Embed vs Search vs Rerank breakdown
- Cost per 1k queries (derived)

> **Recommended minimum**: nDCG@10 + Recall@50 + P95 latency

---

## ðŸ“ Project Structure

```
VoyageAI_TestSuite/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ queries.jsonl         # {query_id, query_text}
â”‚   â”œâ”€â”€ qrels.jsonl           # {query_id, doc_id, grade}
â”‚   â””â”€â”€ corpus/               # Documents stored in Atlas
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ experiment_1_best_direct.py
â”‚   â”œâ”€â”€ experiment_2_best_rerank.py
â”‚   â”œâ”€â”€ experiment_3_qat_rerank.py
â”‚   â””â”€â”€ experiment_4_qat_direct.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ evaluation.py         # Metrics (nDCG, Recall, MRR, MAP)
â”‚   â”œâ”€â”€ atlas_client.py       # MongoDB Atlas Vector Search wrapper
â”‚   â””â”€â”€ logging_utils.py      # Structured experiment logging
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ atlas_config.yaml
â”‚   â””â”€â”€ experiment_config.yaml
â””â”€â”€ results/
    â””â”€â”€ *.json                # Perâ€‘experiment outputs
```

---

## ðŸ§± Oneâ€‘Time Setup (Important)

### 1ï¸âƒ£ Prepare Evaluation Data

- `queries.jsonl`
```json
{"query_id": "q1", "query_text": "How does vector search work?"}
```

- `qrels.jsonl`
```json
{"query_id": "q1", "doc_id": "doc_42", "grade": 2}
```

Grades can be **binary (0/1)** or **graded (0â€“3)**.

> ðŸ’¡ If you donâ€™t have labels, manually labeling **50â€“200 queries** is enough to get strong signal.

---

### 2ï¸âƒ£ Store Documents & Embeddings in Atlas

Example document schema:
```json
{
  "doc_id": "doc_42",
  "text": "Document content here",
  "embedding_best": [...],
  "embedding_qat": [...],
  "metadata": {
    "category": "docs",
    "language": "en"
  }
}
```

Create **two vector search indexes**:
- `vs_best` â†’ `embedding_best`
- `vs_qat` â†’ `embedding_qat`

Everything else must be identical.

---

### 3ï¸âƒ£ Lock Your Retrieval Contract

To keep experiments fair:
- Same chunk size & overlap
- Same similarity metric
- Same `numCandidates` & `limit`
- Same filters & postâ€‘processing

---

## ðŸš€ Running Experiments

```bash
pip install -r requirements.txt

python experiments/experiment_1_best_direct.py
python experiments/experiment_2_best_rerank.py
python experiments/experiment_3_qat_rerank.py
python experiments/experiment_4_qat_direct.py
```

Each run produces:
- Console summary
- JSON metrics
- Perâ€‘query retrieval & rerank logs

---

## ðŸ§ª Results Summary

| Experiment | Description | nDCG@10 | Recall@50 | P95 Total Latency | Notes |
|-----------|-------------|---------|-----------|------------------|-------|
| **Exp 1** | Best Voyage (Direct) | 0.378 | 0.785 | 145 ms | Strong baseline retriever |
| **Exp 2** | Best Voyage + Reranker | **0.421** | **0.785** | 330 ms | Best overall quality |
| **Exp 3** | QAT Voyage + Reranker | 0.409 | 0.772 | 310 ms | Near-best quality, cheaper embeds |
| **Exp 4** | QAT Voyage (Direct) | 0.349 | 0.728 | **115 ms** | Fastest & cheapest |

---

## ðŸ“¦ Requirements

- Python 3.8+
- MongoDB Atlas Vector Search
- VoyageAI API key
- Reranker model access (for experiments 2 & 3)

---

## ðŸ§­ Philosophy

This repo is **not** about chasing leaderboard scores.

Itâ€™s about:
> **Understanding tradeâ€‘offs so you can ship the right system.**

Measure first. Optimize second. Ship confidently.

---

## ðŸ“œ License

MIT License

---

Happy benchmarking ðŸš€  
If your RAG system feels slow, expensive, or mysterious â€” this suite exists to fix that.
